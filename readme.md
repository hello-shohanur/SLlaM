# SLlaM: A Serene Large Language Model

> A minimal transformer-based language model built from scratch with PyTorch

---

## üöÄ Overview

**SLlaM (****Serene Large Language Model****)** is a compact, easy-to-understand implementation of a GPT-style transformer architecture. It's designed for educational purposes, fast prototyping, and small-scale language modeling tasks.

---

## üß† Features

- Fully custom transformer architecture
- Character-level tokenizer (can be extended)
- Supports training and inference (text generation)
- Minimal dependencies (only PyTorch)
- Easy to extend with HuggingFace tokenizer or datasets

---

## üì¶ Installation

```bash
# Clone the repository
$ git clone https://github.com/hello-shohanur/sllam.git
$ cd sllam

# Install requirements
$ pip install torch
```

---

## üèÅ Getting Started

Run the main training and generation script:

```bash
$ python sllam.py
```

### Output:

After training, the model will generate text starting from the character "h".

```
Generated:
 hello world. hello again.hello again.hello again.
```

---

## üõ†Ô∏è File Structure

```
sllam/
‚îú‚îÄ‚îÄ sllam.py         # Main model, training, generation
‚îú‚îÄ‚îÄ README.md        # This file
```

*Note: The **``** file (saved model weights) will be created after training is completed.*

---

## üß™ Usage

You can import and use `SLlaM` in your own projects:

```python
from sllam import SLlaM, SLlaMConfig
```

### Text Generation

```python
context = torch.tensor([[tokenizer.stoi['h']]], device=device)
output = generate(model, context, max_new_tokens=50, tokenizer=tokenizer)
print(output)
```

---

## üìö Customize

You can easily change:

- `vocab_size` ‚Äî for your own tokenization
- `block_size`, `n_layer`, `n_head`, `n_embd` ‚Äî for scaling
- `SimpleTokenizer` ‚Äî replace with real tokenizers like HuggingFace

---

## ü§ñ Model Specs (default)

| Parameter       | Value |
| --------------- | ----- |
| Embedding Dim   | 256   |
| Layers          | 4     |
| Attention Heads | 4     |
| Block Size      | 128   |
| Dropout         | 0.1   |
| Vocab Size      | 10000 |

---

## üìÑ License

This project is MIT licensed. Feel free to use, modify, and share.

---

## üôè Acknowledgements

- Inspired by nanoGPT and GPT architecture
- Built with love using PyTorch

---

## ‚ú® Future Plans

- Tokenizer Enhancements
- Dataset & Training Pipeline
- Training Optimization
- Model Scaling

---

## üîó Connect

Questions? Suggestions? Open an issue or reach out!

---

> Built with ‚ù§Ô∏è by the open-source community.

